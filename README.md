# Literature_research
| Title                                                        | tag                       | Published Year | Institute                                                    | Conference/Journal | Paper Link                                                   | Citation |
| ------------------------------------------------------------ | ------------------------- | -------------- | ------------------------------------------------------------ | ------------------ | ------------------------------------------------------------ | -------- |
| Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey | Survey                    | 2023           | State Key Lab of Novel Software Technology, Nanjing University, China | arxiv              | https://arxiv.org/pdf/2311.12351.pdf                         | 1        |
| Bp-transformer: Modelling long-range context via binary partitioning. | Hierachical Attention     | 2019           | AWS Shanghai AI Lab                                          | arxiv              | https://arxiv.org/pdf/1911.04070.pdf                         | 71       |
| Adaptive attention span in transformers.                     | Hierachical Attention     | 2019           | Facebook AI Research                                         | ACL  CCFA          | https://arxiv.org/pdf/1905.07799.pdf                         | 273      |
| MemGPT: Towards LLMs as Operating Systems                    | OS-like，External MemBank | 2023           | UC Berkeley                                                  | arxiv              | https://arxiv.org/pdf/2310.08560.pdf?isApp=1                 | 10       |
| Memorizing Transformers                                      | Internal MemCache         | 2022           | Google                                                       | ICLR 2022  CCFA    | https://arxiv.org/pdf/2203.08913.pdf                         | 126      |
| Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | Internal MemCache         | 2019           | CMU<br>Google                                                | ACL 2019           | https://arxiv.org/pdf/1901.02860.pdf                         | 3592     |
| Augmenting Language Models with Long-Term Memory             | External MemBank          | 2023           | University of California, Santa Barbara                      | arxiv              | https://arxiv.org/pdf/2306.07174.pdf                         | 23       |
| Compressive transformers for long-range sequence modelling.  | Internal MemCache         | 2019           | DeepMind                                                     | arxiv              | https://arxiv.org/pdf/1911.05507.pdf                         | 398      |
| Langchain                                                    |                           |                |                                                              | arxiv              | https://github.com/langchain-ai/langchain                    |          |
| Memorybank: Enhancing large language models with long-term memory | External MemBank          | 2023           | 中山大学                                                     | arxiv              | https://arxiv.org/pdf/2305.10250.pdf                         | 19       |
| RecallM: An Adaptable Memory Mechanism with Temporal Understanding for<br>Large Language Models | External MemBank          | 2023           | Cisco Systems                                                | arxiv              | https://arxiv.org/pdf/2307.02738.pdf                         | 2        |
| Realm: Retrieval augmented language model pre-training.      | External MemBank          | 2020           | Google                                                       | PMLR               | https://proceedings.mlr.press/v119/guu20a/guu20a.pdf         | 1150     |
| Cogltx: Applying bert to long texts.                         | Context Selection         | 2020           | 清华                                                         | NIPS               | https://proceedings.neurips.cc/paper/2020/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf | 123      |
| Long-span summarization via local attention and content selection. | Context Selection         | 2021           | Department of Engineering, University of Cambridge           | ACL 2021           | https://arxiv.org/pdf/2105.03801.pdf                         | 38       |
| In-context Autoencoder for Context Compression in a Large Language Model | Context Compression       | 2023           | Microsoft                                                    | arxiv              | https://arxiv.org/pdf/2307.06945.pdf                         | 12       |

